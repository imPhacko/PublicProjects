{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imPhacko/PublicProjects/blob/main/Projektas_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ObMcqfl6nBEM",
        "outputId": "19b835ed-e08f-4181-a428-b0ddd7b75c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/DATA AI/TRUMPxBIDEN.txt\"\n",
        "#raw_text = open(filename, \"r\", encoding=\"utf-8\").read()\n",
        "raw_text = open(filename, \"r\", encoding='cp1252').read()\n",
        "raw_text = raw_text.lower()\n",
        "print(raw_text[:200])"
      ],
      "metadata": {
        "id": "pQ4ZCoblnEuS",
        "outputId": "0464f29e-c898-4ffb-af2e-10b1979ace86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moderator: (00:18)\n",
            "good evening, everyone. good evening. thank you so much for being here. it is such an honor for me to moderate this debate tonight, the final debate. i want to welcome the first fam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "outputId": "5c8366b5-e94a-4dfd-a64c-06eae9f902a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 229998 characters\n"
          ]
        }
      ],
      "source": [
        "# decode py2 compat.\n",
        "#raw_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(raw_text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "outputId": "3afbb836-1b89-4376-a14a-c4df93b2bf0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moderator: (00:18)\n",
            "good evening, everyone. good evening. thank you so much for being here. it is such an honor for me to moderate this debate tonight, the final debate. i want to welcome the first family and the first lady. we’re so glad and thankful that you are feeling better. i want to welcome the biden family, dr. jill biden. thank you all for being here tonight. we are so excited. we’re looking forward to a really robust discussion. and the only thing i would reiterate are the cpd guidelines that when the candidates are talking, please hold any applause or any other reactions. except of course, when they walk out, make sure you cheer and loud and applause so that everyone can hear you. thank you for having me. this is really the honor of a lifetime. i am going to sit down and just get organized and get settled and the show will start very soon. thank you for being here. (silence). good evening from belmont university in nashville, tennessee. i’m moderator of nbc news. and i welcome you to the final 2020 presidential debate between donald trump and former joe biden. tonight’s debate is sponsored by the commission on presidential debates. it is conducted under health and safety protocols designed by the commission’s health security advisor. the audience here in the hall has promised to remain silent. no cheers, boos, or other interruptions, except right now, as we welcome to the stage, former joe biden and donald trump.\n",
            "\n",
            "donald trump: (07:37)\n",
            "how are you doing? how are you?\n",
            "\n",
            "moderator: (07:58)\n",
            "and i do want to say a very good evening to both of you. this debate will cover six major topics. at the beginning of each section, each candidate will have two minutes, uninterrupted, to answer my first question. the debate commission will then turn on their microphone only when it is their turn to answer. and the commission will turn it off exactly when the two minutes have expired. after that, both microphones will remain on. but on behalf of the voters, i’m going to ask you to please speak one at a time.\n",
            "\n",
            "moderator: (08:27)\n",
            "the goal is for you to hear each other and for the american people to hear every word of what you both have to say. and so with that, if you’re ready, let’s start. and we will begin with the fight against the coronavirus. president trump, the first question is for you. the country is heading into a dangerous new phase. more than 40,000 americans are in the hospital tonight with covid, including record numbers here in tennessee. and since the two of you last shared a stage, 16,000 americans have died from covid. so please be specific. how would you lead the country during this next stage of the coronavirus crisis? two minutes, uninterrupted.\n",
            "\n",
            "moderator: (09:03)\n",
            "… during this next stage of the coronavirus crisis. two minutes uninterrupted.\n",
            "\n",
            "donald trump: (09:04)\n",
            "so as you know, 2.2 million people modeled out, were expected to die. we closed up the greatest economy in the world in order to fight this horrible disease that came from china. it’s a worldwide pandemic. it’s all over the world. you see the spikes in europe and many other places right now. if you notice, the mortality rate is down 85%. the excess mortality rate is way down and much lower than almost any other country. and we’re fighting it and we’re fighting it hard. there is a spike. there was a spike in florida and it’s now gone.\n",
            "\n",
            "donald trump: (09:41)\n",
            "there was a very big spike in texas. it’s now gone. there was a very big spike in arizona. it’s now gone. and there was some spikes and surges and other places, they will soon be gone. we have a vaccine that’s coming. it’s ready. it’s going to be announced within weeks. and it’s going to be delivered. we have operation warp speed, which is the military is going to distribute the vaccine.\n",
            "\n",
            "donald trump: (10:04)\n",
            "i can tell you from personal experience, i was in the hospital. i had it and i got better. and i will tell you that i had something that they gave me, a therapeutic, i guess they would call it. some people could say it was a cure, but i was in for a short period of time. and i got better very fast or i wouldn’t be here tonight. and now they say i’m immune. whether it’s four months or a lifetime, nobody’s been able to say that, but i’m immune. more and more people are getting better. we have a problem that’s a worldwide problem. this is a worldwide problem, but i’ve been congratulated by the heads of many countries on what we’ve been able to do. if you take a look at what we’ve done in terms of goggles and masks and gowns and everything else, and in particular ventilators we’re now making ventilators all over the world, thousands and thousands a month distributing them all over the world. it will go away. and as i say, we’re rounding the turn. we’re rounding the corner. it’s going away.\n",
            "\n",
            "moderator: (11:06)\n",
            "okay. former vice president biden to you. how would you lead the country out of this crisis? you have two minutes uninterrupted.\n",
            "\n",
            "joe biden: (11:13)\n",
            "220,000 americans dead. yo\n"
          ]
        }
      ],
      "source": [
        "print(raw_text[:5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "outputId": "2db3296c-1c4a-44e1-cad7-493f3b4eea05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(raw_text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##pd.Series(len(x) for x in ' '.join(df.vocab).split()).value_counts().sort_index().plot(kind='bar', figsize=(12, 3))"
      ],
      "metadata": {
        "id": "7pEW-R2M4GHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "outputId": "c72c99f9-5768-43ba-b8d9-30507cbfe835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "outputId": "9a36fa68-aec6-4eb0-b1ca-0eff1c9c393c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[27, 28, 29, 30, 31, 32, 33], [50, 51, 52]]>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "outputId": "09747277-39ca-4ed7-9591-84b0f322da94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "outputId": "f6c617f2-7119-4c1f-8bc9-fc7fbc48605f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "outputId": "d7e25870-a648-4db1-917a-85d8fce8674c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(229998,), dtype=int64, numpy=array([39, 41, 30, ..., 47, 10,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(raw_text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "outputId": "1f6565c7-bf89-4863-ea8a-16105885be1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m\n",
            "o\n",
            "d\n",
            "e\n",
            "r\n",
            "a\n",
            "t\n",
            "o\n",
            "r\n",
            ":\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(raw_text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "outputId": "e059c557-0d23-4cc8-c8f8-7129dccbd5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'm' b'o' b'd' b'e' b'r' b'a' b't' b'o' b'r' b':' b' ' b'(' b'0' b'0'\n",
            " b':' b'1' b'8' b')' b'\\n' b'g' b'o' b'o' b'd' b' ' b'e' b'v' b'e' b'n'\n",
            " b'i' b'n' b'g' b',' b' ' b'e' b'v' b'e' b'r' b'y' b'o' b'n' b'e' b'.'\n",
            " b' ' b'g' b'o' b'o' b'd' b' ' b'e' b'v' b'e' b'n' b'i' b'n' b'g' b'.'\n",
            " b' ' b't' b'h' b'a' b'n' b'k' b' ' b'y' b'o' b'u' b' ' b's' b'o' b' '\n",
            " b'm' b'u' b'c' b'h' b' ' b'f' b'o' b'r' b' ' b'b' b'e' b'i' b'n' b'g'\n",
            " b' ' b'h' b'e' b'r' b'e' b'.' b' ' b'i' b't' b' ' b'i' b's' b' ' b's'\n",
            " b'u' b'c' b'h'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "outputId": "916daf85-d622-41d3-e1b5-df19ed0866ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'moderator: (00:18)\\ngood evening, everyone. good evening. thank you so much for being here. it is such'\n",
            "b' an honor for me to moderate this debate tonight, the final debate. i want to welcome the first famil'\n",
            "b'y and the first lady. we\\xe2\\x80\\x99re so glad and thankful that you are feeling better. i want to welcome the b'\n",
            "b'iden family, dr. jill biden. thank you all for being here tonight. we are so excited. we\\xe2\\x80\\x99re looking f'\n",
            "b'orward to a really robust discussion. and the only thing i would reiterate are the cpd guidelines tha'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "outputId": "bd8609cf-331d-48ad-ef0f-1708165a5525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['D', 'o', 'n', 'a', 'l', 'd', ' ', 'T', 'r', 'u', 'm'],\n",
              " ['o', 'n', 'a', 'l', 'd', ' ', 'T', 'r', 'u', 'm', 'p'])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "split_input_target(list(\"Donald Trump\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "outputId": "c9fdf736-2512-4c30-ec4f-2d7f071d3ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'moderator: (00:18)\\ngood evening, everyone. good evening. thank you so much for being here. it is suc'\n",
            "Target: b'oderator: (00:18)\\ngood evening, everyone. good evening. thank you so much for being here. it is such'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "outputId": "7406e649-3add-4d5d-b95b-c71babc43225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of vocab\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN unit amount\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class Modelis(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = Modelis(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "outputId": "6737d615-9538-4d5d-bb82-b51934021985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 58) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "outputId": "25809f67-7546-40e4-cdd9-aa59c187943f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"modelis\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  14848     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  59450     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,012,602\n",
            "Trainable params: 4,012,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "outputId": "c3d048e3-c192-4ef6-c2fe-4a2899ad459a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 20, 33, 29,  5, 20, 14,  0, 52, 36, 50,  8,  5, 21, 18, 26,  2,\n",
              "       13, 20, 20, 35, 32,  3, 42, 38, 28, 19, 34, 38, 12, 39, 52, 14,  3,\n",
              "       57,  8, 47,  4, 45, 41, 31, 37, 44, 26, 31, 48, 28, 46, 40, 44, 44,\n",
              "       49, 12, 41, 49, 21, 32,  2, 46, 40,  5, 19, 48, 50, 11, 14,  1, 19,\n",
              "       41,  7, 22, 13, 21, 31, 12,  0, 56,  5, 45, 22, 48, 12,  6,  8, 14,\n",
              "       32, 18, 27, 49, 13, 21, 13, 33, 39, 10,  7,  0,  7, 32, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "outputId": "68e417bf-3ec5-4743-9163-edab7130fc78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'. and with regard to more divided the nation, it can\\xe2\\x80\\x99t stay divided. we can\\xe2\\x80\\x99t be this way. and speak'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'h8gc&82[UNK]zjx,&96] 188if$plb7hl0mz2$\\xe2\\x80\\xa6,u%soekr]evbtnrrw0ow9f tn&7vx/2\\n7o):19e0[UNK]\\xe2\\x80\\x9d&s:v0(,2f6aw191gm.)[UNK])fc'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "outputId": "fb412b31-b3f1-49e6-fc3e-e801b4dd5dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 58)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.0597, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "outputId": "81b9e689-a017-48aa-ee04-1b2fa79002d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57.95692"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "j60kG1JeWC8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "outputId": "4ee6df17-ef3a-4c7b-8416-06fdd4ccf958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "35/35 [==============================] - 5s 134ms/step - loss: 0.4387\n",
            "Epoch 2/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.4031\n",
            "Epoch 3/40\n",
            "35/35 [==============================] - 5s 135ms/step - loss: 0.3716\n",
            "Epoch 4/40\n",
            "35/35 [==============================] - 5s 131ms/step - loss: 0.3377\n",
            "Epoch 5/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.3073\n",
            "Epoch 6/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.2811\n",
            "Epoch 7/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.2576\n",
            "Epoch 8/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.2392\n",
            "Epoch 9/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.2211\n",
            "Epoch 10/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.2055\n",
            "Epoch 11/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1933\n",
            "Epoch 12/40\n",
            "35/35 [==============================] - 5s 134ms/step - loss: 0.1807\n",
            "Epoch 13/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1700\n",
            "Epoch 14/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1606\n",
            "Epoch 15/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1534\n",
            "Epoch 16/40\n",
            "35/35 [==============================] - 5s 136ms/step - loss: 0.1464\n",
            "Epoch 17/40\n",
            "35/35 [==============================] - 5s 135ms/step - loss: 0.1398\n",
            "Epoch 18/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1338\n",
            "Epoch 19/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.1295\n",
            "Epoch 20/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.1245\n",
            "Epoch 21/40\n",
            "35/35 [==============================] - 5s 136ms/step - loss: 0.1198\n",
            "Epoch 22/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.1161\n",
            "Epoch 23/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1126\n",
            "Epoch 24/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1116\n",
            "Epoch 25/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1095\n",
            "Epoch 26/40\n",
            "35/35 [==============================] - 5s 134ms/step - loss: 0.1078\n",
            "Epoch 27/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1045\n",
            "Epoch 28/40\n",
            "35/35 [==============================] - 5s 134ms/step - loss: 0.1021\n",
            "Epoch 29/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1009\n",
            "Epoch 30/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1004\n",
            "Epoch 31/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0985\n",
            "Epoch 32/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0982\n",
            "Epoch 33/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0985\n",
            "Epoch 34/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0990\n",
            "Epoch 35/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0982\n",
            "Epoch 36/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.0987\n",
            "Epoch 37/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1000\n",
            "Epoch 38/40\n",
            "35/35 [==============================] - 5s 133ms/step - loss: 0.1020\n",
            "Epoch 39/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.1058\n",
            "Epoch 40/40\n",
            "35/35 [==============================] - 5s 132ms/step - loss: 0.1095\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "outputId": "ac37ffb4-d40e-4aeb-f898-4134b88634d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moderator: (50:48)\n",
            "all right. this is a guy who in fact need the he pood because you don’t have to come back every year.” “we have to come back.”\n",
            "\n",
            "donald trump: (18:25)\n",
            "he’s going to pask the coronavirus. president trump, the first question is for you. you’ll each have one minute, starting with you, mr. trump.\n",
            "\n",
            "donald trump: (42:08)\n",
            "well, chank within we knew. we got the masks. we made the vent lank in the stame, but you do that.\n",
            "\n",
            "joe biden: (14:20)\n",
            "because they were kere, and he plans hose back to the questions obamacare, both community.\n",
            "\n",
            "donald trump: (24:46)\n",
            "he tried it in his state.\n",
            "\n",
            "joe biden: (14:59)\n",
            "number two.\n",
            "\n",
            "donald trump: (56:03)\n",
            "now, having a get fire drag actuary of went and their first under obama asked for because i said, “i think you need more.” and just god demenations and presidential now, by the way, was given it. i do. i do it to them all. and by the way, my son…\n",
            "\n",
            "moderator: (46:19)\n",
            "wait a minute. y. gever people are dying. and he sumpond i’m going to eliminated, it’s going to be over soon.” come on. there’s not another serious scientist in the world we don’t have much of the united states is a got to this threat? you have two minutes.\n",
            "\n",
            "joe biden: (09:12)\n",
            "the plat i probode so man fact he knows how to do-\n",
            "\n",
            "donald trump: (01:10)\n",
            "excuse me. i got trouble. mary sending on the following badly runding our severmies. it makes up and have a system where people are held accountable when… and anyone who committed it should be prosecuted.\n",
            "\n",
            "moderator: (40:02)\n",
            "but you’ve never called for the last 15 om north servisto millions of jobs, by the mildions of dollars. [crosstalk 00:58:13] the laptop is now another republicans after that. that was becare in a very happy to have plexigmasar and indistration will to determine the outcome of this election. if in fact you can verify that’s you before very just like he has done. he has caused the deficit of the china to go up, not do changes and counties running. and they befone with obama. you said that’s going on because our businesses are protection rate went oner jobu, with regard to this drug, compare me to the senatelm squentisns.\n",
            "\n",
            "moderator: (01:01:55)\n",
            "yes. i don’t want to talk about the science of climate change, why have you rolled back the obama clean people.” that’s why i’ve been arguinits. it was a way of spending on this drbase. the other thing that’s on the conordarizetime, we have a plan.\n",
            "\n",
            "moderator: (17:03)\n",
            "mr. president, please some of people agree with the recovery act, i want to talk about both of your records, but your response to that law, that it doesn’t would agree. you talk about police, pigs, pigs, talking about our police. “pigs in a blanket,” talking about police, “pigs in a believe of the number of people and hard to get any word in with this clown. excuse me, medicaid would automabily hurt pollutay and he blaw environmental regulations have hingle for thems and we have to get in the just report responders and they can do but look at these massive windmills of people a day are controbrating now. it was a terrible thing. as far as my rebuilt the vaccine.\n",
            "\n",
            "donald trump: (18:25)\n",
            "he’s going to pack the court. he is not going to give a job done by rais. i know how to do the job. i know how to do the job. i mean, i have a mask a number of worse accordunt to have socialized medicine, just on economy. he ballots in the world with other countries in rach time, we’re going to allo that you want to ketw of you-\n",
            "\n",
            "moderator: (02:55)\n",
            "all right. deynvinu is. making 30 foces and counties in iowa. they didn’t happen before. they’re in facilities that were so clean.\n",
            "\n",
            "moderator: (34:58)\n",
            "president trump, wait a minute.\n",
            "\n",
            "moderator: (22:06)\n",
            "china ate your lunch- [crosstalk 00:32:35]-\n",
            "\n",
            "moderator: (31:29)\n",
            "you said very recently-\n",
            "\n",
            "joe biden: (31:35)\n",
            "no.\n",
            "\n",
            "moderator: (13:38)\n",
            "just 30 seconds here because we need to get on the everything else has no out a thery inliverssar. and your family, and joe does to play by the rules. and what’s he do? he embraces guys like the thu casal. of che passo, if the proud us i get buck be months. the final question. with right now, name oir electric es cle an opportunity to have healthcare for their children. how have never come up with a setting and he does but i also thing we’re going to work this out.\n",
            "\n",
            "joe biden: (16:03)\n",
            "when?\n",
            "\n",
            "joe biden: (20:33)\n",
            "no.\n",
            "\n",
            "moderator: (20:33)\n",
            "okay. president trump, thank you. quick response vice president biden. you’ve said you wouldn’t have closed in for as a presidentia. they were very happy this. [crosstalk 00:21:34)\n",
            "we got rid of a militerce at a mask and social defter. they have a plan.\n",
            "\n",
            "donald trump: (10:34)\n",
            "they were sterifically are emergece these ballots are going to be all over. take at a low standard, and we kicked in right away, i was a reproduction rate i ran. i alter edrected to hunt their insurance unless they fores back to their lives.\n",
            "\n",
            "joe biden: (10:54)\n",
            "that’s the end of the sets of those parents for because of him, when my mom get if run it said. you’ll tell you what so you impairante to deal with the environment, they’re like a vaccunize what he’s talking about your family, and just 40 menuig to the stock market. when the stock market goes up, that means jobs. it also talk about it. tried to help them do excupp in the health and your son gave you, and then it was determined of thousands of leas notra-\n",
            "\n",
            "donald trump: (23:00)\n",
            "excuse me. he was there-\n",
            "\n",
            "moderator: (23:01)\n",
            "president trump, i’d like to continue with the outcome. and i would like you to let me answere bef easl radicelol small business.\n",
            "\n",
            "moderator: (40:33)\n",
            "go ahead sir.\n",
            "\n",
            "joe biden: (48:20)\n",
            "make sure it’s totally to let me answer? because of her. not 705, well, it’s his ownoh, former vice president? he has no idea what he’s talking about. [crosstalk 00:17:14].\n",
            "\n",
            "moderator: (17:31)\n",
            "all right. i want to welcome the big dent from any place, including it house rass and go tark about it.\n",
            "\n",
            "donald trump: (48:04)\n",
            "if we just were there in america, and i want to talk about the way black and brown americans exsence. the vote has been substantial interfereate in ukraine. i made it, chris.\n",
            "\n",
            "moderator: (09:22)\n",
            "president trump, on china policy-\n",
            "\n",
            "joe biden: (21:15)\n",
            "that’s not true. it doesn’t want to talk about the way black and brown americans exactly winh. it was contidual mandate would take their nome small bessue and a half. i was a way i don’t think he’s talking to them a lot. i don’t think he made a fortune and he didn’t want us… and what did he do?\n",
            "\n",
            "joe biden: (40:54)\n",
            "show us your tax returns.\n",
            "\n",
            "donald trump: (13:54)\n",
            "that’s a big statement.\n",
            "\n",
            "joe biden: (23:25)\n",
            "well if you let that happen, joe. you know that.\n",
            "\n",
            "joe biden: (00:00)\n",
            "making sure that they got the done. he’s talking about [crosstalk 00:35:15]-\n",
            "\n",
            "joe biden: (02:51)\n",
            "by the way, i have a follow-up for you vice president? you keep ballots are about the clevilanes taxings and we can defree of without the parents. they come over through of politically. we can’t get new tirest?\n",
            "\n",
            "donald trump: (08:54)\n",
            "well, let’s have to come back every year.” “we have to come back.”\n",
            "\n",
            "donald trump: (08:08)\n",
            "well, so far, i saye it was a were doing. he said we owe him debt of grief and they shot him for three days portland didn’t do anything. i see to respond to that before we move on.\n",
            "\n",
            "donald trump: (57:06)\n",
            "that’s ros try badlet all of the court. if i think you a specific, well, as far as they’re concerned.\n",
            "\n",
            "joe biden: (10:33)\n",
            "number two, i don’t make money from russia.\n",
            "\n",
            "donald trump: (26:48)\n",
            "and he doesn’t come from scranton. he lived there families, but that’s a big question.\n",
            "\n",
            "moderator: (20:54)\n",
            "all right. i’m going to let the vice-president respond to that quickly, and then i need to get to your economic plan would create 7 million more americans have fallen intepriputions and worgh. so they quillion to elections in a row now, they refuse to talk about it-\n",
            "\n",
            "joe biden: (23:51)\n",
            "because they went with it. i never said it.\n",
            "\n",
            "moderator: (58:24)\n",
            "all right, i want to thank you both for a very high rate.\n",
            "\n",
            "joe biden: (08:29)\n",
            "number two, i don’t make money from russia.\n",
            "\n",
            "donald trump: (06:04)\n",
            "you know joe, i ran because of you. i ran because of you. i ran because of you. i ran because of you. i ran because of you. i ran because of you. i ran because of you. i ran because of you. i ran because offy… if you want to just quickly finish up? because i want to move on to our final section. president obama saying that they were paying people hundreds of thousands of acres on a late. rull and ruisbons for that election and cheas and have d sponders, they wouldn’t even think they tried because they had no chance at all? orabity saying we could look at all, california they were paying you and have a sense of sects.\n",
            "\n",
            "moderator: (05:52)\n",
            "let him finish, sir.\n",
            "\n",
            "joe biden: (55:24)\n",
            "we’re going to pas canning from and true. that in fact people talking about this? that’s not what it is about. he’s talking about defunding the police.\n",
            "\n",
            "joe biden: (08:23)\n",
            "he wants to shut down this cirisulay renamodard the olly one of the most successful propes and it’s going undept. probably strife more about wind than you do.” i don’t think the privile fewer and they’re going to have forest management. in advictred transparencay in american history. i was asked to vote eare clost up working on i was with it. we have happen in the world with sou, you need, everything. joe. if you would have had the character of this country. decency, honor, respect. treated care of stuffes by the president and i… i think healthcare is not. [crosstalk 00:27:10] matter is i beat bernie sanders.\n",
            "\n",
            "donald trump: (14:14)\n",
            "if you were a certain person, you had more are going to be all over. take a look at what happened in new jobs. that’s the big new taxes on individuals making more than she’l plat how to do the job. i know how to do the job. we can do that anyone hear accountant to wall street, because you’re going to have to extremely money. our characte, and then they went withing the autional rassian pollutes, significantly.\n",
            "\n",
            "donald trump: (42:31)\n",
            "well, this is what i-\n",
            "\n",
            "moderat \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 39.790870666503906\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['moderator: '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(10000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}